\documentclass{memoir}
\usepackage{mystyle}

\begin{document}

\setcounter{section}{10}
\setcounter{dfn}{14}

\subsection*{Application: Solving linear matrix equations}

In the last application, we computed a basis for the kernel of a linear transformation given by a matrix: $A(v) = Av$. Another way to characterize this is to say we computed the solution of the equation $Av = 0$; the solution is precisely the span of that basis. At first blush the slightly altered equation $Av = c$ might seem not to be amenable to a similar approach - the solution of this equation is not a subspace, for instance, unless $c = 0$, and so it doesn't make sense to find a basis for it. Note, however, that \[ Av = c \quad \mathrm{if\ and\ only\ if}\ \left[ \begin{array}{c|c} A & \text{-}c \end{array} \right] \left[ \begin{array}{c} v \\ \hline [1] \end{array} \right] = 0. \] In other words, the solution of the matrix equation $Av = c$ is contained in the kernel of the ``augmented'' linear transformation $[A \mid \text{-}c]$; in fact, it is precisely (all but the last row of) the vectors in that kernel whose last row is $[1]$.

Let $M = [\mu_{i,j}]$ of dimension $n \times m$ be the reduced row echelon form of $[A \mid c]$, and let $(1,\ell_1), \ldots, (k,\ell_k)$ be the pivots of $M$. As we showed earlier, the vectors \[  z_j = d_j - \sum_{i=1}^k \mu_{i,j} d_{\ell_i}, \] where $j$ ranges over the indices of the nonpivotal columns of $M$ and $d_j$ is the $j$th standard basis vector, forms a basis for the kernel of $M$. In particular, if the last column of $M$ is pivotal, then every element of $\Ker*{M}$ has a 0 in the last row. So no vectors in $\Ker*{M}$ have the necessary form to be in the solution of our equation.

Suppose instead that the last column of $M$ is nonpivotal. Let us write the indices of the nonpivotal columns of $M$ by $t_1, \ldots, t_{m-k}$. Then the kernel of $M$ is precisely all vectors of the form \[ \alpha_1 z_{t_1} + \alpha_2 z_{t_2} + \cdots + \alpha_{m-k-1} z_{t_{m-k-1}} + \alpha_{m-k} z_{t_{m-k}}. \] By definition, the $m$th row of each $z_{t_i}$ is 0 except for the last one, $z_{t_{m-k}}$. So an element of $\Ker*{M}$ is in our solution set precisely when the last coefficient $\alpha_{m-k}$ is 1. That is, the solution of the equation $Av = c$ is precisely the first $m-1$ entries of the vectors of the form \[ \alpha_1 z_{t_1} + \alpha_2 z_{t_2} + \cdots + \alpha_{m-k-1} z_{t_{m-k-1}} + z_{t_{m-k}}. \]

\subsection*{Example}

Consider the equation \[ Av = \begin{bmatrix} 1 & 0 & 2 \\ 0 & 1 & 3 \end{bmatrix} v = \begin{bmatrix} -1 \\ 4 \end{bmatrix} = c. \] In this case the augmented matrix \[ M = \left[ \begin{array}{ccc|c} 1 & 0 & 2 & 1 \\ 0 & 1 & 3 & -4 \end{array}\right] \] has two nonpivotal columns, yielding the basis \[ z_3 = \begin{bmatrix} -2 \\ -3 \\ 1 \\ 0 \end{bmatrix} \quad \mathrm{and} \quad z_4 = \begin{bmatrix} -1 \\ 4 \\ 0 \\ 1 \end{bmatrix} \] for $\Ker*{M}$. Fixing a second coordinate of $\alpha_4 = 1$, the solution of $Av = c$ is precisely the first 3 rows of the vectors of the form \[ \alpha_3 z_3 + z_4 = \begin{bmatrix} -2\alpha_3 - 1 \\ -3 \alpha_3 + 4 \\ \alpha_3 \\ 1 \end{bmatrix} \quad \mathrm{where}\ \alpha_3 \in \mathbb{Q}. \]

\subsection*{Application: Detecting invertible matrices and computing inverses}

Given an arbitrary $n \times n$ matrix $A$ we have a natural question with a natural follow-up challenge:
\begin{enumerate*}
\item Is $A$ invertible?
\item If so, exhibit $A^\inv$.
\end{enumerate*}
The reduced row echelon form can help us with both of these.

\begin{prp}
If $A$ is an $m \times n$ matrix, then
\begin{enumerate*}
\item The pivotal columns of $A$ are independent and
\item The nonpivotal columns of $A$ are dependent on the pivotal columns.
\end{enumerate*}
In particular, the pivotal columns are a basis for $\Im*{A}$.
\end{prp}

\begin{proof}
Write the Gauss-Jordan factorization of $A$ as $A = PR$. Note that the columns of $A$ are precisely the images of the columns of $R$ under the bijective linear transformation $P$. Since the pivotal columns of $R$ are independent and $P$ is injective, the pivotal columns of $A$ are independent, and similarly (ii) follows.
\end{proof}

\begin{prp}
If $A$ is an $n \times n$ matrix, then the following are equivalent.
\begin{enumerate*}
\item $A$ is invertible.
\item $A$ has $n$ pivots.
\item The reduced row echelon form of $A$ is $I$.
\end{enumerate*}
\end{prp}

\begin{proof} \mbox{}
\begin{enumerate*}
\item[(i) $\Rightarrow$ (iii)] Suppose $A$ is invertible, and write the Gauss-Jordan factorization of $A$ as $A = PR$. Now $A^\inv P R = I$. But note that $R$ and $I$ are reduced row echelon matrices which are row equivalent, so that $R = I$.

\item[(iii) $\Rightarrow$ (ii)] Clearly $I$ has $n$ pivots.

\item[(ii) $\Rightarrow$ (i)] If $A$ has $n$ pivotal columns, then $\Im*{A}$ has dimension $n$. So $A$ is surjective and hence bijective. \qedhere
\end{enumerate*}
\end{proof}

Part (ii) is the useful one here: to determine whether or not $A$ is invertible, it suffices to compute a row echelon matrix row equivalent to $A$. This can be done relatively efficiently.

If $A$ is invertible, computing $A^\inv$ is a different problem. But part (iii) above suggests one way to do it: the matrix $Q$ such that $QA = I$ is in reduced row echelon form is precisely $A^\inv$. By our proof, we can find $Q$ using elementary row operations. And in fact, it is enough to row-reduce $[A \mid I]$, since \[ Q[A \mid I] = [QA \mid Q] = [I \mid Q]. \]

In fact this gives us a little more. If $A$ is invertible, then its Gauss-Jordan factorization is $A = PI$, where $P = A$. But in proving the existence of a Gauss-Jordan factorization we saw that the invertible matrix $P$ could be written as a product of elementary matrices. Thus we have

\begin{cor}
Every invertible matrix is a product of elementary matrices. Or: the group of invertible $n \times n$ matrices is generated by the $n \times n$ elementary matrices.
\end{cor}

\subsection*{Application: Given a gen set, find a min gen set}

Every set $S$ of $n \times 1$ matrices generates a subspace, and moreover, $S$ contains a \emph{basis} for $\Span*{S}$. Can we find one? Recall that the pivotal columns of a matrix are independent, and the nonpivotal columns are dependent on the pivotal columns. We considered this statement before as being about the columns of a given matrix. But we can turn this around to effectively find a minimal generating subset of a given generating set. Given a set $S$ of vectors, make $S$ the columns of a matrix $A$. The pivotal columns of $A$ are a minimal generating set (i.e. basis) for $\Span*{S}$. As a special case, if every column of $A$ is pivotal then $S$ is independent.

\subsubsection*{Example}

We claim that the set \[ S = \left\{ v_1 = \begin{bmatrix} 1 \\ 4 \\ 6 \end{bmatrix}, v_2 = \begin{bmatrix} -3 \\ 2 \\ 2 \end{bmatrix}, v_3 = \begin{bmatrix} 1 \\ 9 \\ -2 \end{bmatrix} \right\} \] is independent. To see this it suffices to note that the reduced row echelon form of the matrix \[ A = \left[ \begin{array}{c|c|c} v_1 & v_2 & v_3 \end{array} \right] = \begin{bmatrix} 1 & -3 & 1 \\ 4 & 2 & 9 \\ 6 & 2 & -2 \end{bmatrix}, \] as computed with the algorithm presented earlier, is $I$.

\subsection*{Application: Decide whether $w$ is in $\Span*{S}$}

Given a set $S = \{v_1,\ldots,v_n\}$ and a vector $w$, we have a natural question with a natural follow-up challenge:
\begin{enumerate*}
\item Is $w$ in $\Span*{S}$?
\item If so, exhibit $\alpha_i$ such that $w = \sum_{i=1}^n \alpha_i v_i$.
\end{enumerate*}

Note that \[ \left[ \sum_{i=1}^n \alpha_i v_i \right] = \left[ \begin{array}{c|c|c|c} v_1 & v_2 & \cdots & v_n \end{array} \right] \left[ \begin{array}{c} [\alpha_1] \\ \hline [\alpha_2] \\ \hline \vdots \\ \hline [\alpha_n] \end{array} \right]. \] In particular, if we make the vectors in $S$ the columns of a matrix $A = \left[ \begin{array}{c|c|c|c} v_1 & v_2 & \cdots & v_n \end{array} \right]$, then the linear combinations of $S$ are precisely the matrices $Ax$ where $x$ is a column matrix. So $w$ is in $\Span*{S}$ precisely when the equation $Ax = w$ has a solution, and moreover the solutions of this equation are precisely the $\alpha_i$ which witness this.

\subsubsection*{Example}

Let \[ S = \left\{ v_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}, v_2 = \begin{bmatrix} -3 \\ 4 \end{bmatrix}, v_3 = \begin{bmatrix} -2 \\ -2 \end{bmatrix} \right\} \quad \mathrm{and} \quad w = \begin{bmatrix} 12 \\ -10 \end{bmatrix}, \] and let \[ [A \mid \text{-}w] = \left[ \begin{array}{c|c|c|c} v_1 & v_2 & v_3 & \text{-}w \end{array} \right] = \begin{bmatrix} 1 & -3 & -2 & -12 \\ 2 & 4 & -2 & 10 \end{bmatrix}. \] Using row reduction, ``we can see'' that the reduced row echelon form of $[A \mid \text{-}w]$ is \[ M = \begin{bmatrix} 1 & 0 & -7/5 & -9/5 \\ 0 & 1 & 1/5 & 17/5 \end{bmatrix}. \] Thus we have \[ \left(\frac{7\alpha + 9}{5}\right) v_1 + \left(\frac{-\alpha - 17}{5}\right)v_2 + \alpha v_3 = w \] where $\alpha \in \mathbb{Q}$.

\end{document}